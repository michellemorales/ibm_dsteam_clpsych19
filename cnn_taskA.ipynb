{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CLPsych19 Shared Task\n",
    "For questions contact Michelle.Morales@ibm.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Load data and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T10:42:05.162399Z",
     "start_time": "2019-03-15T10:40:22.484857Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from preprocess import *\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm, metrics\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.utils import to_categorical\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T10:42:57.617800Z",
     "start_time": "2019-03-15T10:42:57.573929Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Optional steps depending on where your scripts and data are\n",
    "# os.getcwd()\n",
    "os.chdir('clpsych19_training_data/') # point directory to training data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T10:43:25.527457Z",
     "start_time": "2019-03-15T10:43:02.048285Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Data for Task A\n",
    "label_data = pd.read_csv('crowd_train.csv')\n",
    "subreddit_data = pd.read_csv('task_A_train.posts.csv')\n",
    "text_data = pd.read_csv('shared_task_posts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T10:47:25.651002Z",
     "start_time": "2019-03-15T10:47:14.938983Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Merge dataframes\n",
    "sub_label_data = pd.merge(subreddit_data, label_data, how = 'left', on = 'user_id')\n",
    "data = pd.merge(sub_label_data, text_data, on = ['post_id', 'user_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T10:47:50.204683Z",
     "start_time": "2019-03-15T10:47:29.894915Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Preprocess\n",
    "data = data.fillna('')\n",
    "join_title_and_body(data)\n",
    "data['text'] = data.apply(lambda x: to_lower_case(x['text']), axis=1)\n",
    "# data['text'] = data.apply(lambda x: remove_punc(x['text']), axis=1)\n",
    "data['text'] = data.apply(lambda x: remove_(x['text']), axis=1)\n",
    "\n",
    "# Transform df from post to user level\n",
    "text_by_user = data.groupby(['user_id'])['text'].apply(lambda x: ' '.join(x)).reset_index()\n",
    "text_df = pd.merge(text_by_user, label_data, how = 'left', on = 'user_id')\n",
    "text_df['tokens'] = text_df.apply(lambda x: tokenize(x['text']), axis=1) # Tokenize text\n",
    "text_df['tokens'] = text_df.apply(lambda x: lemmatize(x['tokens']), axis=1) # Lemmatize tokens\n",
    "text_df['text'] = text_df['tokens'].str.join(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Load in word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T10:53:17.312876Z",
     "start_time": "2019-03-15T10:53:12.086053Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "wv_model = Word2Vec.load('word2vec_reddit.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T10:54:28.861032Z",
     "start_time": "2019-03-15T10:53:44.374288Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Retrofit them using code from: https://github.com/mfaruqui/retrofitting\n",
    "retro_model = KeyedVectors.load_word2vec_format('retro_word2vec_reddit.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T10:56:23.684418Z",
     "start_time": "2019-03-15T10:56:23.657419Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    retro_model['suicide']\n",
    "except KeyError:\n",
    "    print(KeyError)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T11:10:55.451154Z",
     "start_time": "2019-03-15T11:10:55.440192Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def tokens2wordvec(tokens, model):\n",
    "    vecs = []\n",
    "    for t in tokens:\n",
    "        try:\n",
    "            wv = model[t]\n",
    "            vecs.append(wv)\n",
    "        except KeyError:\n",
    "            continue\n",
    "    if vecs == []:\n",
    "        avg_vec = [0]*100\n",
    "    elif vecs != []:\n",
    "        avg_vec = np.mean(vecs, axis=0)\n",
    "    return avg_vec        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T11:11:00.797818Z",
     "start_time": "2019-03-15T11:10:57.224891Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "text_df['word2vec'] = text_df.apply(lambda x: tokens2wordvec(x['tokens'], wv_model), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T11:21:58.918720Z",
     "start_time": "2019-03-15T11:21:54.181421Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "text_df['retro_word2vec'] = text_df.apply(lambda x: tokens2wordvec(x['tokens'], retro_model), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T11:34:28.964336Z",
     "start_time": "2019-03-15T11:34:28.913066Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "text_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Baseline (SVM) system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T11:42:26.274108Z",
     "start_time": "2019-03-15T11:42:25.527499Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# count_vect = CountVectorizer(stop_words='english', analyzer='word') # System 1\n",
    "count_vect = CountVectorizer(stop_words='english', analyzer='word', ngram_range=(1, 2), min_df=.1, max_df=.8) # System 2\n",
    "X_train_counts = count_vect.fit_transform(text_df['text'])\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts) # Already scaled between 0-1 no need to scale for SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T11:42:27.150305Z",
     "start_time": "2019-03-15T11:42:27.146126Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X = X_train_tfidf\n",
    "y =  text_df['raw_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T11:42:28.794151Z",
     "start_time": "2019-03-15T11:42:28.655357Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=1000, stratify=y)\n",
    "clf = svm.SVC(kernel='linear')\n",
    "clf.fit(X_train, y_train)\n",
    "predicted = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T11:44:47.956293Z",
     "start_time": "2019-03-15T11:44:47.939194Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(metrics.classification_report(y_test, predicted))\n",
    "print(metrics.accuracy_score(y_test, predicted))\n",
    "print(metrics.precision_recall_fscore_support(y_test, predicted, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### CNN system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T11:38:54.478198Z",
     "start_time": "2019-03-15T11:38:54.443469Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X = X_train_tfidf\n",
    "one_hot = pd.get_dummies(text_df['raw_label'])\n",
    "target_labels = one_hot.columns\n",
    "target = one_hot.as_matrix()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, target, test_size=0.20, random_state=1000, stratify=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T11:38:58.161863Z",
     "start_time": "2019-03-15T11:38:57.328387Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "input_dim = X_train.shape[1]  # Number of features\n",
    "cnn_model = Sequential()\n",
    "cnn_model.add(layers.Dense(10, input_dim=input_dim, activation='relu'))\n",
    "cnn_model.add(layers.Dense(4, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T11:39:00.705770Z",
     "start_time": "2019-03-15T11:39:00.596789Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cnn_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T11:39:07.332447Z",
     "start_time": "2019-03-15T11:39:03.610350Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "history = cnn_model.fit(X_train, y_train,\n",
    "                    epochs=30,\n",
    "                    verbose=False,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T11:39:09.615976Z",
     "start_time": "2019-03-15T11:39:09.580470Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "loss, accuracy = cnn_model.evaluate(X_train, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = cnn_model.evaluate(X_test, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T11:39:13.314415Z",
     "start_time": "2019-03-15T11:39:13.252562Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Evaluate F1-score\n",
    "y_pred = cnn_model.predict_classes(X_test)\n",
    "transformed_y_test = [l.tolist().index(1) for l in y_test]\n",
    "metrics.f1_score(y_pred, transformed_y_test, average='macro')\n",
    "metrics.precision_recall_fscore_support(y_pred, transformed_y_test, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Plot loss and accuracy to help with optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-14T15:49:43.462781Z",
     "start_time": "2019-03-14T15:49:43.437220Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "\n",
    "def plot_history(history):\n",
    "    acc = history.history['acc']\n",
    "    val_acc = history.history['val_acc']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    x = range(1, len(acc) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(x, acc, 'b', label='Training acc')\n",
    "    plt.plot(x, val_acc, 'r', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(x, loss, 'b', label='Training loss')\n",
    "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-14T15:49:55.281114Z",
     "start_time": "2019-03-14T15:49:54.352173Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:clpsych19]",
   "language": "python",
   "name": "conda-env-clpsych19-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}