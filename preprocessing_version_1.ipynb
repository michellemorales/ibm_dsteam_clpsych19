{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('stopwords') \n",
    "# nltk.download('words')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import words as common_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "from symspellpy.symspellpy import SymSpell, Verbosity\n",
    "\n",
    "def loadCommonDictionary():\n",
    "    try:\n",
    "        words = common_words.words()\n",
    "    except OSError:\n",
    "        print(\"Common Word Dictionary Not Found\")\n",
    "    else:\n",
    "        return words\n",
    "\n",
    "def loadComplexDictionary():\n",
    "    try: \n",
    "        dictionary_path = os.path.join(os.getcwd(), \"symspellpy_frequency_dist.txt\") \n",
    "        return dictionary_path\n",
    "    except OSError:\n",
    "        print(\"File Not Found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "symObj = SymSpell(max_dictionary_edit_distance = 2, prefix_length = 7, compact_level = 2)\n",
    "commonWords = loadCommonDictionary()\n",
    "dictionary = loadComplexDictionary()\n",
    "symObj.load_dictionary(dictionary, term_index = 0, count_index = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd() #'C:\\\\Users\\\\ThomasTheisen\\\\Documents\\\\suicidewatchdata'\n",
    "mylist = [f for f in glob.glob(\"*.csv\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['post_id', 'user_id', 'timestamp', 'subreddit', 'post_title', 'post_body']\n",
    "_data = pd.DataFrame(columns=columns)\n",
    "for file in mylist:\n",
    "    d = pd.read_csv(file)\n",
    "    data = _data.append(d, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "def utc_to_real(utc_ts):\n",
    "    return datetime.utcfromtimestamp(int(utc_ts)).strftime('%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_title_and_body(dataset):\n",
    "    cols = ['post_title', 'post_body']\n",
    "    dataset['text'] = dataset[cols].apply(lambda row: '_'.join(row.values.astype(str)), axis=1)\n",
    "    dataset.drop(cols, inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_URLS(text):\n",
    "    return re.sub(r'http\\S+', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_lower_case(text):\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_numbers(text):\n",
    "    return ''.join([i for i in text if not i.isdigit()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punc(text):\n",
    "    return re.sub(r'[^\\w\\s]','', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_(text):\n",
    "    return text.replace('_',' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_repeat_words(text):\n",
    "    toRemove = []\n",
    "    prev = None\n",
    "    split_text = text.split()\n",
    "    for index, word in enumerate(split_text):\n",
    "        if prev == word:\n",
    "            toRemove.append(index)\n",
    "        prev = word\n",
    "    for index in sorted(toRemove, reverse=True):\n",
    "        del split_text[index]\n",
    "    return ' '.join(split_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    return ' '.join([word for word in text.split() if word not in stopwords.words('english')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complex_correction(word):\n",
    "    suggestions = symObj.lookup(phrase = word, verbosity = Verbosity.CLOSEST, max_edit_distance = 1)\n",
    "    return suggestions\n",
    "\n",
    "def spell_check(text):\n",
    "    split_text = text.split()\n",
    "    for index, word in enumerate(split_text):\n",
    "        if word not in commonWords:\n",
    "            suggestions = complex_correction(word)\n",
    "            if len(suggestions) != 0:\n",
    "                split_text[index] = split_text[index].replace(word, suggestions[0].term)\n",
    "    return ' '.join(split_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['timestamp'] = data.apply(lambda x: utc_to_real(x['timestamp']), axis=1)\n",
    "join_title_and_body(data)\n",
    "data['text'] = data.apply(lambda x: remove_URLS(x['text']), axis=1)\n",
    "data['text'] = data.apply(lambda x: to_lower_case(x['text']), axis=1)\n",
    "data['text'] = data.apply(lambda x: remove_numbers(x['text']), axis=1)\n",
    "data['text'] = data.apply(lambda x: remove_punc(x['text']), axis=1)\n",
    "data['text'] = data.apply(lambda x: remove_(x['text']), axis=1)\n",
    "data['text'] = data.apply(lambda x: remove_repeat_words(x['text']), axis=1)\n",
    "data['text'] = data.apply(lambda x: spell_check(x['text']), axis=1)\n",
    "data['text'] = data.apply(lambda x: remove_stopwords(x['text']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "# nltk.download('wordnet')\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "def lemma_to_verb(text):\n",
    "    split_text = text.split()\n",
    "    for index, word in enumerate(split_text):\n",
    "        lemma = wordnet_lemmatizer.lemmatize(word, 'v')\n",
    "        split_text[index] = lemma\n",
    "    return ' '.join(split_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data.apply(lambda x: lemma_to_verb(x['text']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['class'] = np.where(data['subreddit'] == 'SuicideWatch', 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
