{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CLPsych19 Shared Task\n",
    "For questions contact Michelle.Morales@ibm.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T19:14:30.220263Z",
     "start_time": "2019-03-07T19:14:30.212355Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from preprocess import *\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "##### Load data & preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T21:52:24.583174Z",
     "start_time": "2019-03-06T21:52:24.574705Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Optional steps depending on where your scripts and data are\n",
    "# os.getcwd()\n",
    "# os.chdir('clpsych19_training_data/') # point directory to training data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T18:19:56.154894Z",
     "start_time": "2019-03-07T18:19:32.624357Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Data for Task A\n",
    "label_data = pd.read_csv('crowd_train.csv')\n",
    "subreddit_data = pd.read_csv('task_A_train.posts.csv')\n",
    "text_data = pd.read_csv('shared_task_posts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T23:29:16.379087Z",
     "start_time": "2019-03-07T23:29:11.016001Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Merge dataframes\n",
    "sub_label_data = pd.merge(subreddit_data, label_data, how = 'left', on = 'user_id')\n",
    "data = pd.merge(sub_label_data, text_data, on = ['post_id', 'user_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T23:25:20.775801Z",
     "start_time": "2019-03-07T23:25:20.741520Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Get info about the data\n",
    "print(data.shape)\n",
    "print(data.columns)\n",
    "print(data['user_id'].value_counts().describe())\n",
    "print(data['raw_label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "##### Baseline system: preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T23:29:23.390650Z",
     "start_time": "2019-03-07T23:29:20.883007Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Preprocess\n",
    "data = data.fillna('')\n",
    "join_title_and_body(data)\n",
    "data['text'] = data.apply(lambda x: to_lower_case(x['text']), axis=1)\n",
    "data['text'] = data.apply(lambda x: remove_punc(x['text']), axis=1)\n",
    "data['text'] = data.apply(lambda x: remove_(x['text']), axis=1)\n",
    "\n",
    "# Transform df from post to user level\n",
    "text_by_user = data.groupby(['user_id'])['text'].apply(lambda x: ' '.join(x)).reset_index()\n",
    "text_df = pd.merge(text_by_user, label_data, how = 'left', on = 'user_id')\n",
    "text_df['tokens'] = text_df.apply(lambda x: tokenize(x['text']), axis=1) # Tokenize text\n",
    "text_df['tokens'] = text_df.apply(lambda x: lemmatize(x['tokens']), axis=1) # Lemmatize tokens\n",
    "text_df['text'] = text_df['tokens'].str.join(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "##### Split data, build model, and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T23:45:52.629195Z",
     "start_time": "2019-03-07T23:45:51.361034Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Set up stratified 5 fold cross-validation\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "\n",
    "# count_vect = CountVectorizer(stop_words='english', analyzer='word') # System 1\n",
    "count_vect = CountVectorizer(stop_words='english', analyzer='word', ngram_range=(1, 2), min_df=.1, max_df=.8) # System 2\n",
    "X_train_counts = count_vect.fit_transform(text_df['text'])\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts) # Already scaled between 0-1 no need to scale for SVM\n",
    "\n",
    "X = X_train_tfidf\n",
    "y =  text_df['raw_label']\n",
    "skf = StratifiedKFold(n_splits=5, random_state=30, shuffle=False) # Make sure to use the same random state number!\n",
    "skf.get_n_splits(X, y)\n",
    "\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    clf = svm.SVC(kernel='linear')\n",
    "    clf.fit(X_train, y_train)\n",
    "    predicted = clf.predict(X_test)\n",
    "    scores = metrics.precision_recall_fscore_support(y_test, predicted, average='macro')\n",
    "    precision_scores.append(scores[0])\n",
    "    recall_scores.append(scores[1])\n",
    "    f1_scores.append(scores[2])\n",
    "    print(metrics.classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T23:45:56.532318Z",
     "start_time": "2019-03-07T23:45:56.519905Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(\"Precision = {}\".format(np.mean(precision_scores)))\n",
    "print(\"Recall = {}\".format(np.mean(recall_scores)))\n",
    "print(\"F1-score = {}\".format(np.mean(f1_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T23:46:00.793098Z",
     "start_time": "2019-03-07T23:46:00.779555Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "len(count_vect.get_feature_names())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:clpsych19]",
   "language": "python",
   "name": "conda-env-clpsych19-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}